\section{Implementation Details} \label{sec:impl}

\subsection{The \paxos Protocol} \label{sec:paxos}
The \paxos consensus component (\S\ref{sec:abstraction}) is a critical 
component to enforce a consistent total order of socket calls from client 
programs. Although there are already a number of open source \paxos 
implementations~\cite{concoord, zookeeper, libpaxos}, we re-implemented a \paxos 
protocol in order to incorporate our new socket-API consensus interface.

Our \paxos implementation is based on a well-known and concise 
approach~\cite{paxos:practical}. In normal case, only the primary invokes 
consensus, thus this approach reaches consensus efficiently. In exceptional 
cases such as primary restarts, a \paxos leader election 
is invoked to resolve conflicts. In \xxx, we implemented this election via 
making the primary send a heart beat message to all the backups every 
second, and if backup replicas have not receive any heart beat message for 
three seconds, these replicas start to elect a new leader. The 
leader election contains three steps~\cite{paxos:practical}: (1) backups 
proposing a new view, which is a standard \paxos two-phase 
consensus~\cite{paxos:simple}, (2) the proposer that wins the view proposing 
itself as a primary candidate, another standard \paxos two-phase consensus, and 
(3) the new leader announcing itself as the new primary.

In our implementation, each socket call from 
the client is assigned a global, monotonically increasing viewstamp (or 
\emph{global index}) to associate with each checkpoint 
(\S\ref{sec:checkpoint}). Upon consensus on a socket call, each consensus 
component persistently stores the call type, arguments, and global index into a 
Berkeley DB storage~\cite{berkeleydb} on SSD.

Although our current \paxos implementation focuses on supporting socket 
consensus interface, this \paxos protocol logic is independent of the types and 
arguments of socket operations, so our \paxos implementation can be applied 
to other types of consensus interface as well.

\subsection{Checkpoint and Restore} \label{sec:checkpoint}
To recover or add a new replica, \xxx leverages two popular open source 
tools: \criu~\cite{criu}, to checkpoint process state such as CPU registers and 
memory; and \lxc~\cite{lxc}, to checkpoint the file system state of a server 
program's current working directory and installation directory. These two 
directories are sufficient to capture files modified by the server programs in 
our evaluation.

Incorporating \lxc into \xxx has two extra practical benefits. First, the 
server process is ran within an \lxc, which provides the server the same 
and clean initial systems state and mitigates contentions on systems resources
(\eg, file descriptors) with other processes. Second, \lxc snapshots make \xxx 
easy to deploy on multiple replicas without worrying about slight differences of 
the systems environments such as kernel and library versions. We just built \xxx 
on one replica once, did a \lxc snapshot, and then copied the snapshot to 
other replicas.

A \xxx checkpoint operation contains three steps. First, \xxx 
uses \criu to checkpoint the server's process running within the \lxc 
container and dumps the checkpoint to the process's current working directory. 
\criu needs to modify systems files (\eg, ns\_last\_pid), but \lxc's 
default isolation configuration does not permit these modifications, so we 
configure \lxc to run in ``unconfined mode". Second, \xxx stops the container, 
uses ``\v{diff --text}" to generate a patch of current working directory and 
the server's installation directory against an \lxc snapshot prepared before 
any server starts. This file system checkpoint patch is incremental and 
thus efficient (\S\ref{sec:recovery}). Third, \xxx restarts the container, and 
restores the server process with \criu.

Such a \xxx checkpoint operation is done every minute on one 
backup replica without affecting the other replicas' performance. We 
explicitly design \xxx's proxy and consensus component stateless and they do not 
require checkpoints. A \xxx restore operation reverts these steps.

One main issue on checkpointing a server process is that it constantly accepts 
socket connections, but checkpointing and restoring 
TCP stacks are notoriously difficult. Our trick to avoid this difficulty is 
based on an observation: even busy server programs have some idle moments. For 
instance, consider \apache, even running with its standard performance-stress 
benchmark \ab, we observed that in some moments the server has no alive socket 
connections. Thus, during a checkpoint operation, \xxx simply checks whether the 
server has alive connections. If so, \xxx backs off for a few seconds and then 
retries until the server has no alive connections. Since checkpoint periods do 
not have to be precise, this trick runs well (\S\ref{sec:recovery}). 
\section{\xxx's Synchronization Wrappers for a Server} \label{sec:wrappers}

This section describes how \xxx handles a server program's 
synchronizations, including \pthread synchronizations and blocking socket 
calls. Because how to handle these synchronizations is tightly relevant 
to the \parrot \dmt scheduler we leverage, in this section, we first introduce 
some background on the \parrot scheduler, including its primitives and wrappers. 
And then we describe how \xxx leverages \parrot's primitives and wrappers to 
implement its own synchronization wrappers.

\subsection{Background: the \parrot Scheduler} \label{sec:parrot}

\begin{figure}[t]
\centering
\begin{minipage}{.5\textwidth}
\lgrindfile{code/dmt-interface.cpp}
\end{minipage}
\vspace{-.1in}
\caption{{\em The \parrot \dmt runtime's scheduler primitives.}} 
\label{fig:dmt-primitives}
\vspace{-.05in}
\end{figure}

\parrot~\cite{parrot:sosp13} is a \dmt system that uses the \ldpreload trick to 
intercept \pthread synchronizations at runtime and enforces a 
well-define, round-robin order for these operations. In this round-robin 
manner, \parrot first lets one runnable thread do one synchronization 
operation; and then, for the left runnable threads, \parrot lets the next 
thread do one synchronization operation; and then the next runnable thread, 
until all runnable threads having done one synchronization operation. Then 
\parrot repeats. To enforce this schedule, \parrot maintains a queue of runnable 
threads (\emph{run queue}) and another queue of waiting threads (\emph{wait 
queue}), like a Linux OS scheduler.

\parrot enforces an important invariant: only the thread at the head of the run 
queue can do one actual synchronization operation and manipulate the run queue 
and wait queue. After the head thread does one operation, it rotates itself 
to the tail of the run queue and wakes up the new head thread of the run queue. 
Conceptually, threads within \parrot pass a global token (the run queue head) 
around. A thread will be put into the wait queue if the synchronization object 
it requires is not available, and it will be put back to the run queue 
when this object becomes available.

\begin{figure}[t]
\centering
\begin{minipage}{.5\textwidth}
\lgrindfile{code/lock.cpp.lineno}
\end{minipage}
\vspace{-.1in}
\caption{{\em \parrot's wrapper for \mutexlock.}} 
\label{fig:lock}
% \vspace{-.05in}
\end{figure}

To implement this round-robin schedule in a compact way, \parrot provides a 
monitor-like internal interface, shown in Figure~\ref{fig:dmt-primitives}. The 
\getturn function waits until the calling thread becomes the head of the run 
queue. The \putturn function rotates the calling thread to the tail of the run 
queue and wakes up the next thread which now is the head of the run queue. The 
\wait function puts the calling thread from run queue to wait queue and blocks 
on a opaque object (\eg, a mutex or a socket descriptor), until another 
thread makes this object available and calls a \signal on this object. When a 
thread returns from a \wait function, it becomes the head of the run queue. 
Both the \wait and the \signal functions require getting the global turn.

These set of primitives are highly optimized for multi-core. Each thread has an 
integer flag and condition variable. The \getturn function spin-waits on the 
current thread's flag for a while before blocking on the condition variable. 
The \wait function needs to get the turn before it returns, so it uses the same 
combined spin- and block- wait strategy as the \getturn function. The \getturn 
and \signal functions signal both the flag and the conditional variable of the 
next thread. In common case, these operations acquire no lock and do not 
block-wait, thus the number of synchronization context switches in \parrot is 
much smaller than that in traditional \pthread synchronizations, yielding 
faster performance in \parrot than in the \pthread runtime for some 
programs~\cite{parrot:sosp13}.

% For instance, \parrot's evaluation~\cite{parrot:sosp13} showed that, when 
% transcoding an OSDI 2012 presentation video using a popular parallel transcoder 
% \mencoder, \parrot only incurred 921 context switches, while the traditional 
% Linux \pthread runtime incurred 1.9M context switches, leading to a 
% \mencoderspeedup speedup of \mencoder with \parrot.

Figure~\ref{fig:lock} shows the \mutexlock wrapper in \parrot. This wrapper 
uses try-lock to avoid deadlock: if the head of the run queue is blocked waiting 
for a lock before giving up the turn, no other thread can get the turn.

When all threads of a program block, which is common case in a server 
program, \parrot puts an internal \emph{idle thread} to the run queue, which 
simply does repetitive \getturn and \putturn operations. This idle thread 
ensures that \parrot's run queue always has threads and that \parrot's logical 
clock keeps ticking.

\parrot's blocking socket calls are nondeterministic because it 
is a \dmt system for eliminating nondeterminism in \pthread 
synchronizations. A blocking socket call's wrapper in \parrot works as 
follows. When a thread calls a blocking socket call, the thread 
calls \getturn, passes the global token to the next thread in the run 
queue, removes itself from the run queue, and then calls into the 
actual socket call. When the thread returns from the actual call, it appends 
itself to a \emph{socket queue}. Each thread at the run queue head moves the 
threads in this socket queue back to the run queue. This move-back is 
nondeterministic because threads may return from blocking socket calls 
nondeterministically and thus may be added to the socket queue in various 
orders.
% However, \parrot's invariant, only the thread at run queue 
% head can manipulate the run queue, is maintained.

\subsection{\xxx' Synchronization Wrappers for a Server}
\label{sec:socket-wrappers}

\begin{figure}[t]
\centering
\begin{minipage}{.5\textwidth}
\lgrindfile{code/check-timebubble.cpp.lineno}
\end{minipage}
\vspace{-.1in}
\caption{{\em \xxx's \checktimebubble function.}} 
\label{fig:checktimebubble}
\vspace{-.10in}
\end{figure}

\xxx wraps a rich set of common blocking socket operations, including \select, 
\poll, \epollwait, \accept, and \recv. \xxx also modifies the wrappers of 
\pthread synchronizations. These wrappers are sufficient for the server 
programs in our evaluation.

% These wrappers enforce the same logical clocks for 
% server programs' blocking operations and \pthread synchronizations across 
% different replicas and do the actual synchronization operations.

\xxx needs to modify the \mutexlock wrapper in Figure~\ref{fig:lock} to do 
three things. First, if the \paxos request sequence has been empty for a 
physical duration \ntimeout, \xxx requests a time bubble with \nclock logical 
clocks. Second, if the head of the \paxos sequence is a time bubble, \xxx 
decreases the logical clock in the time bubble by one, or it removes this bubble 
if zero clock is left. Third, \xxx signals a thread that blocks on a socket 
operation (\eg, \recv) if there is a matching client socket call (\eg, \send) at 
the head of the \paxos sequence. To do these three things, \xxx calls the 
\checktimebubble function (defined in Figure~\ref{fig:checktimebubble}) at Line 
3 of the \mutexlock wrapper in Figure~\ref{fig:lock}.

An important data structure in \xxx's wrapper is the \paxos sequence which 
contains clients' socket calls and inserted time bubbles. This sequence sits 
between the proxy and the server's processes, and it is implemented with 
Boost~\cite{boost} shared memory. \xxx uses \v{lockf()} to ensure mutual 
exclusion on this sequence because the two processes may concurrently 
manipulate this sequence. For clarity, these lock and unlock operations are 
omitted in the pseudo code.

\begin{figure}[t]
\centering
\begin{minipage}{.5\textwidth}
\lgrindfile{code/recv.cpp.lineno}
\end{minipage}
\vspace{-.1in}
\caption{{\em \xxx's wrapper for \recv.}} 
\label{fig:recv}
\vspace{-.05in}
\end{figure}

\xxx also needs to modify \parrot's idle thread mechanism because sometimes 
this thread is the only thread in the run queue, and \xxx needs to frequently 
check whether a new client socket call comes or a time bubble insertion is 
needed. To do so, \xxx replaces \parrot's \getturn and \putturn primitives 
within the idle thread to be mutex lock and unlock operations, then the idle 
thread also runs the function defined in Figure~\ref{fig:checktimebubble} to 
check and insert time bubbles.

% Introduce the poll wrapper. This is tricky because instead of letting 
% Unlike \parrot, \xxx's blocking socket calls of a server program across 
% replicas can be scheduled at consistent logical times with the \paxos sequence 
% of client socket calls and the inserted time bubbles. 

Figure~\ref{fig:recv} shows \xxx's wrapper for the \recv call. This wrapper 
ensures that the \recv calls of server programs across replicas return at 
consistent logical times. The other blocking socket calls' wrappers are 
similar. A thread calling \recv in \xxx simply calls \getturn and blocks on the 
socket descriptor using \parrot's \wait primitive. When a client 
\send call that matches this \recv becomes the head of the \paxos sequence, 
the \mutexlock wrappers wakes up the server thread blocking on \recv with the
\signal call at Line 9 in Figure~\ref{fig:checktimebubble}. The waken up thread 
dequeues a number of matching \send calls from the \paxos sequence according to 
the actual bytes received. Also, for clarity, the lock and unlock operations for 
the \paxos sequence are omitted in this \recv wrapper.


\section{The Time Bubbling Technique} \label{sec:time-bubble}

\begin{figure}[t]
\centering
\includegraphics[width=.5\textwidth]{figures/time-bubble-flow}
\vspace{-.40in}
\caption{{\em The request and time bubble flow.}} \label{fig:bubbleflow}
\vspace{-.05in}
\end{figure}

Figure~\ref{fig:bubbleflow} shows the time bubbles inserted by the \timealgo 
technique. The technique groups clients' socket operations as bursts. A request 
burst can be a group of real socket requests (rectangles), or can be a time 
bubble with a fixed number of logical clocks (circles). In this figure, black 
requests are the first operation for each burst.


In a conceptual level, \xxx uses three rules to enforce the same sequence of 
logical times for socket requests (rectangles) and thus the same schedules 
across different replicas. First, \xxx uses \paxos to ensure the same 
sequence of client socket calls as well as inserted time bubbles as a 
``\paxos request sequence" for each replica, as shown in each horizontal arrow. 
Second, \xxx uses \dmt to guarantee that it only ticks logical clocks (\ie, 
schedules \pthread synchronizations or socket operations) when this 
sequence is not empty. Third, the \timealgo technique ensures that this 
sequence is not empty, otherwise it inserts a time bubble.


Figure~\ref{fig:handlebubble} shows the work flow of our \timealgo technique 
with four steps. Each replica's \dmt just waits for a physical duration 
\ntimeout, if no further requests come, (1) the \dmt requests its own proxy to 
insert a time bubble. (2) The proxy then checks whether it sees itself as the 
primary in the \paxos protocol. If so, it asks (3) the consensus component to 
invoke consensus on whether inserting this bubble; otherwise it drops this 
request. After a consensus on this bubble insertion is reached, (4) each 
machine's proxy simply inserts the bubble into the \paxos sequence, granting 
\nclock logical clocks to the \dmt scheduler.


If a server has not exhausted the logical clocks in a time bubble after 
serving current requests, \parrot's idle thread mechanism 
(\S\ref{sec:parrot}) exhausts these clocks rapidly. 
Then, the server can continue to process further requests in time.

\begin{figure}[t]
\centering
\includegraphics[width=.5\textwidth]{figures/handle-time-bubble}
\vspace{-.30in}
\caption{{\em The work flow of inserting a time bubble.}} 
\label{fig:handlebubble}
\vspace{-.1in}
\end{figure}

% Note that even given the same sequence of real socket operations, across 
% different executions, a server program running in \xxx may still run into 
% different schedules because \dmt only waits for a short amount of time (Line 5 
% in Figure~\ref{fig:lock}) and then requests time bubbles and a SIGUSR2 signal 
% across processes may get lost. However, across different machines, the servers 
% still run the same schedules because \paxos ensures that the same amount of 
% time bubbles as well as real socket requests for the \dmt schedulers across 
% replica nodes.


